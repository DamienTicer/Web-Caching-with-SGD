# Comparing Knapsack-Based Stochastic Gradient Descent Approach To Typical Web Caching Methods

## ğŸ“„ Project Summary

This project evaluates and compares the effectiveness of a **Knapsack-Based Stochastic Gradient Descent (SGD) Approach** to traditional web caching methods such as **LRU (Least Recently Used)**, **LFU (Least Frequently Used)**, and **Greedy Knapsack** strategies.

The pipeline simulates web requests, applies different caching algorithms, records cache hit rates, latency reduction, and cache usage, then summarizes and visualizes the results.

---

## ğŸ“‚ Project Structure Overview

```
â”œâ”€â”€ interim_data/           # Temporary and intermediate data files
â”‚   â”œâ”€â”€ request_data.csv
â”‚   â”œâ”€â”€ processed_request_data.csv
â”‚   â”œâ”€â”€ optimized_cache_selection.csv
â”‚   â”œâ”€â”€ cache_results.json
â”‚   â””â”€â”€ web_resources.json
â”‚
â”œâ”€â”€ logs/                   # Logs and detailed iteration outputs
â”‚   â”œâ”€â”€ flask_server.log
â”‚   â”œâ”€â”€ cache_baselines_output.txt
â”‚   â””â”€â”€ comprehensive_metrics.txt
â”‚
â”œâ”€â”€ result_data/            # Processed data summaries
â”‚   â”œâ”€â”€ cumulative_performance_metrics.csv
â”‚   â””â”€â”€ average_performance_metrics.csv
â”‚
â”œâ”€â”€ result_visuals/         # Visual outputs of analysis
â”‚   â”œâ”€â”€ cache_usage_percentage_iterations.png
â”‚   â”œâ”€â”€ latency_reduction_iterations.png
â”‚   â””â”€â”€ cache_hit_rate_iterations.png
â”‚
â”œâ”€â”€ server.py               # Flask server to simulate content requests
â”œâ”€â”€ simulate_requests.py    # Generates random web requests and logs them
â”œâ”€â”€ data_preprocessing.py   # Processes request data (assigns size/latency)
â”œâ”€â”€ sgd_cache_optimizer.py  # Applies SGD-based caching strategy
â”œâ”€â”€ cache_baselines.py      # Runs baseline caching strategies
â”œâ”€â”€ analyze_results.py      # Analyzes single iteration results
â”œâ”€â”€ metric_summary_analysis.py # Aggregates and visualizes metrics over all iterations
â””â”€â”€ pipeline_automation.py  # Full pipeline to automate the entire process
```

---

## âš™ï¸ Dependencies Installation

Ensure you have **Python 3.9+** installed.

Install required packages:

```
py -m pip install flask numpy pandas matplotlib scikit-learn redis tqdm requests
```

---

## â–¶ï¸ Running the Full Pipeline

To run the entire automated process:

```bash
py pipeline_automation.py
```

Once the pipeline completes, execute the summary analysis:

```bash
py metric_summary_analysis.py
```

This will generate cumulative metrics, average metrics, and visualizations under `result_data/` and `result_visuals/`.

---

## ğŸ”„ Running Individual Components (Optional)

If you want to run each script manually, execute them in this order:

```bash
py server.py               # Run Flask server in separate terminal
py simulate_requests.py    # Simulate request data
py data_preprocessing.py   # Assign size and latency to requests
py sgd_cache_optimizer.py  # Optimize cache using SGD
py cache_baselines.py      # Apply baseline caching methods
py analyze_results.py      # Analyze current iteration
py metric_summary_analysis.py # Summarize all iterations
```

---

## ğŸ“Œ Notes
- `pipeline_automation.py` will automatically clean `logs/comprehensive_metrics.txt` at the start.
- All logs, data files, and result images will be organized into their respective folders.
- Each iteration result is appended to cumulative metrics and averaged at the end.